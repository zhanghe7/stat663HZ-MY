{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_processor_em(documents):\n",
    "    stopWords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    articles = []\n",
    "    word_id = {}\n",
    "    id_word = {}\n",
    "    current_wordid = 0\n",
    "    wordID = []\n",
    "    Countword = []\n",
    "    singlecount = [] \n",
    "    store1 = []\n",
    "    store2 = []\n",
    "    for doc in documents:\n",
    "        word_count = {}\n",
    "            ## split words\n",
    "        word_split = doc.split()\n",
    "        for word in word_split:\n",
    "            word = word.lower().strip()\n",
    "            ##\n",
    "            if (len(word)) > 1 and  word.isalpha() and word not in stopWords:\n",
    "                if word not in word_id:\n",
    "                    word_id[word] = current_wordid\n",
    "                    id_word[current_wordid] = word\n",
    "                    current_wordid += 1\n",
    "                if word in word_count:\n",
    "                    word_count[word] += 1\n",
    "                else:\n",
    "                    word_count[word] = 1\n",
    "                \n",
    "        wordIDList = []\n",
    "        wordCountList = []\n",
    "        wordCount = 0\n",
    "\n",
    "        for word in word_count.keys():\n",
    "            wordIDList.append(word_id[word])\n",
    "            wordCountList.append(word_count[word])\n",
    "            wordCount = wordCount + word_count[word] #total words in a doc\n",
    "        wordID.append(wordIDList)\n",
    "        Countword.append(wordCount)\n",
    "        singlecount.append(wordCountList)\n",
    "        store1.append(wordIDList)\n",
    "        store1.append(wordCountList)\n",
    "        store1.append(wordCount)    \n",
    "        store1 = []\n",
    "        store2.append(store1)\n",
    "    return wordID,Countword,singlecount,word_id,id_worda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization and EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxwordnum():\n",
    "    length = []\n",
    "    for l in wordID:\n",
    "        length.append(len(l))\n",
    "    return max(length)   \n",
    "\n",
    "def initial():\n",
    "    for z in range(0, K):\n",
    "        for w in range(0, N):\n",
    "            ntw[z, w] += 1.0/N + np.random.random() # probabily of word under every topic\n",
    "            nt[z] += ntw[z, w] \n",
    "    updatebeta(beta,K,N, ntw, nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import cython\n",
    "cimport numpy as np\n",
    "import numpy as np\n",
    "from libc.math cimport log\n",
    "cimport scipy.special.cython_special\n",
    "from scipy.special.cython_special import psi\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.cdivision(True)\n",
    "\n",
    "def updatebeta(double[:,:] beta, int K,int N, double[:,:] ntw, double[:] nt):\n",
    "    \n",
    "    cdef int z,w\n",
    "    for z in range(0,K):\n",
    "        for w in range(0,N):\n",
    "            if(ntw[z,w] > 0):\n",
    "                beta[z,w] = beta[z,w] = log(ntw[z,w] / nt[z])\n",
    "            else:\n",
    "                beta[z,w] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import cython\n",
    "cimport numpy as np\n",
    "import numpy as np\n",
    "from libc.math cimport exp,log\n",
    "cimport scipy.special.cython_special\n",
    "from scipy.special.cython_special import psi\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.cdivision(True)\n",
    "\n",
    "def variational(double[:,:] gamma, double[:,:] phi, singlecount,wordID, Countword, double[:,:] beta,\n",
    "                int alpha, int d, int K, int infer_iter):\n",
    "    \"\"\"\n",
    "    Variational with E step\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma         : a matrix, shape(M,K), latent parameter of a distribution of topic for each document\n",
    "    phi           : a matrix, shape(K,v), latent parameter of a distribution of words for each topic\n",
    "    wordID        : a list returned by text_processor method\n",
    "    Countword     : a list returned by text_processor method\n",
    "    beta          : a matrix, returned by initial method\n",
    "    alpha         : a fix number \n",
    "    d             : a fix number \n",
    "    K             : number of topics, a fix number\n",
    "    infer_iter    : number of iteration\n",
    "    \n",
    "    Returns\n",
    "    -------gamma\n",
    "    gamma         : updated gamma, a matrix, shape(M,K), distribution of topic for each document\n",
    "   \n",
    "   \"\"\"\n",
    "\n",
    "    cdef int wordlength = len(wordID[d])\n",
    "    cdef double totalphi = 0\n",
    "    cdef double[:] phi_old = np.zeros((K))\n",
    "    cdef double [:] gamma_prime = np.zeros((K))\n",
    "    cdef int z, iteration,w\n",
    "\n",
    "    # initialize parameter\n",
    "    for z in range(K):\n",
    "        #initialize gamma\n",
    "        gamma[d][z] = alpha + Countword[d] * 1.0 / K\n",
    "        gamma_prime[z] = psi(gamma[d][z])\n",
    "        # initialize phi\n",
    "        for w in range(0,len(wordID[d])):\n",
    "            phi[w,z] = 1.0 / K\n",
    "    \n",
    "    for iteration in range(infer_iter):\n",
    "        for w in range(wordlength):\n",
    "            totalphi = 0\n",
    "            for z in range(0,K):\n",
    "                phi_old[z] = phi[w,z]\n",
    "                phi[w,z] = beta[z,wordID[d][w]] + gamma_prime[z]\n",
    "                if z > 0:\n",
    "                    totalphi = log((exp(totalphi)) + exp(phi[w,z]))\n",
    "                else:\n",
    "                    totalphi = phi[w,z]\n",
    "            for z in range(0,K):\n",
    "                phi[w,z] = exp(phi[w,z] - totalphi)\n",
    "                gamma[d][z] = gamma[d][z] + singlecount[d][w] * (phi[w,z] - phi_old[z])\n",
    "                gamma_prime[z] = psi(gamma[d][z]) \n",
    "                \n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perp = []\n",
    "for iteration in range(0,EM_iter):\n",
    "    nt = np.zeros((K))\n",
    "    ntw = np.zeros((K,N))\n",
    "    alphass = 0\n",
    "    \n",
    "        # E step\n",
    "    for d in range(0,M):\n",
    "        variational(gamma, phi, singlecount,wordID, Countword, beta, alpha, d, K, infer_iter)\n",
    "        totalgamma = 0\n",
    "        for z in range(0,K):\n",
    "            totalgamma += gamma[d,z]\n",
    "            alphass += psi(gamma[d,z])\n",
    "        alphass -= K * psi(totalgamma)\n",
    "        \n",
    "        for w in range(len(wordID[d])):\n",
    "            for z in range(0,K):\n",
    "                ntw[z][wordID[d][w]] += singlecount[d][w] * phi[w,z]\n",
    "                nt[z] += singlecount[d][w] * phi[w,z]\n",
    "    # M step\n",
    "    updatebeta(beta,K,N, ntw, nt)\n",
    "    \n",
    "    # perplexity\n",
    "    nd = np.sum(np.array(gamma),1)\n",
    "    n = 0\n",
    "    ll = 0.0\n",
    "\n",
    "    for d,doc in enumerate(wordID):\n",
    "        for w in doc:\n",
    "            ll = ll + np.log(((np.array(ntw)[:, w] / np.array(nt))* (np.array(gamma)[d, :] / nd[d])).sum())\n",
    "            n = n + 1\n",
    "            exp_ll = np.exp(ll/(-n))\n",
    "    perp.append(exp_ll) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
